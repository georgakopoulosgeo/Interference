/project
  ├── run_social_network.sh
  ├── coordinator.py             # Take input from the user (TestCase, Interference) and coordinate the test
  ├── workload_run_monitors.py   # Functions to run the workload and create the workload_metrics.csv from the workload output
  ├── container_monitor.py       # Fucntions to start/stop container monitoring and store metrics in container_metrics.csv
  ├── system_monitor.py          # Functions to start/stop system monitoring and store metrics in system_metrics.csv
  ├── Results
    │   ├── workload_metrics.csv   # Metrics from the workload run (These are application-level metrics)
    │   ├── container_metrics.csv  # Metrics from the container monitoring
    │   ├── system_metrics.csv     # Metrics from the system monitoring
  ├── VM_Workload_Test_Cases.csv


// Format of CSVs
workload_metrics:

container_metrics:

system_metrics:

// Prometheus
ssh -L 9090:localhost:9090 ubuntu@147.102.13.70


4. Create a new .csv using the formulas:
 Total_Bytes_Read = (F_demand + F_hw_prefetch + F_sw_prefetch) × Cache_Line_Size
where:
• F_demand is the count from ls_dmnd_fills_from_sys.mem_io_local
• F_hw_prefetch is the count from ls_hw_pf_dc_fills.mem_io_local
• F_sw_prefetch is the count from ls_sw_pf_dc_fills.mem_io_local
• Cache_Line_Size is typically 64 bytes
Total_Bytes_Writes ≈ (Store_Count) × (Average_Write_Size)
Where:  
• Store_Count is the count from ls_dispatch.store_dispatch
• Average_Write_Size is the typical size of each write (e.g., 8 bytes for a 64-bit operation)
IPC = (instructions) / (cpu-cycles)

all the other information will be stored as they were monitored.
(Keep in mind that this data is high resolution, meaning that for each )